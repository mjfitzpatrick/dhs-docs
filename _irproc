From valdes@noao.edu Tue Mar  2 17:17:03 2004
X-Autogenerated: Mirror
X-Mirrored-by: <fitz@noao.edu>
X-Spam-Checker-Version: SpamAssassin 2.61 (1.212.2.1-2003-12-09-exp) on  noao.edu
X-Spam-Status: No, hits=-4.8 required=4.5 tests=AWL,BAYES_00 autolearn=ham  version=2.61
X-Spam-Level: 
X-TFF-CGPSA-Version: 1.2.7
X-TFF-CGPSA-Filter: Scanned
X-Real-To: <fitz@noao.edu>
Date: Tue, 2 Mar 2004 17:16:59 -0700 (MST)
From: Frank Valdes <valdes@noao.edu>
To: mario_prog@puppis.tuc.noao.edu
Subject: IRPROC Whitepaper
Cc: fitz@puppis.tuc.noao.edu, shaw@puppis.tuc.noao.edu,
   mdickinson@puppis.tuc.noao.edu
X-Sun-Charset: US-ASCII

The recent IR and NEWFIRM training tutorial put me to thinking at home
about the key elements of a NEWFIRM pipeline and data handling system.
I needed to write down my ideas while they were in my mind.  The
concept that I was thinking about was an optimized program to handle
the basic instrumental steps including the difficult problem of
tracking the background with a moving median.  With my experience with
CCDPROC, the Mosaic Data Handling System, and the distributed Mosaic
Pipeline I think the IRPROC processing server is a powerful and
implementable concept that I describe in the whitepaper referenced
below; it is really more of a design sketch.  The whitepaper may be
reached through a recently added DPP NEWFIRM Project Page:

http://www.noao.edu/iraflocal/Projects/Newfirm/
(account iraf, password iraftwg)

The whitepaper direct link (also requiring the account login) is

http://www.noao.edu/iraflocal/Projects/Newfirm/irproc.html


I hope no one think me presumptous for thinking and writing down ideas
such as this.

Frank

From fpierfed@noao.edu Wed Mar  3 11:13:37 2004
X-Autogenerated: Mirror
X-Mirrored-by: <fitz@noao.edu>
X-Spam-Checker-Version: SpamAssassin 2.61 (1.212.2.1-2003-12-09-exp) on  noao.edu
X-Spam-Status: No, hits=-4.8 required=4.5 tests=AWL,BAYES_00 autolearn=ham  version=2.61
X-Spam-Level: 
X-TFF-CGPSA-Version: 1.2.7
X-TFF-CGPSA-Filter: Scanned
X-Real-To: <fitz@noao.edu>
From: Francesco Pierfederici <fpierfed@noao.edu>
Subject: Fwd: IRPROC Whitepaper
Date: Wed, 3 Mar 2004 09:24:16 -0700
To: Dick Shaw <shaw@noao.edu>, Mike Fitzpatrick <fitz@noao.edu>,
   mdickinson@puppis.tuc.noao.edu


Ops! I forgot to copy you when I sent my reply to Frank's message.

f

Begin forwarded message:

> From: Francesco Pierfederici <fpierfed@noao.edu>
> Date: March 2, 2004 8:25:18 PM MST
> To: mario_prog@noao.edu (Pipeline Programmers)
> Subject: Re: IRPROC Whitepaper
> Reply-To: mario_prog@noao.edu (Pipeline Programmers)
>
>
> Hi everybody,
>
> I would like to add few thoughts to the discussion.
>
> The first idea is that Frank's architecture (based on processing 
> servers) allows for pretty darn good performance. Reading each image 
> from disk every time can, easily, become a problem. Some simple math 
> suggest that we could be spending a reasonable fraction of the time it 
> takes to produce an image, simply reading files from disk (1). 
> Besides, the memory requirements of this architecture, per CCD, are 
> not huge (2).
>
> Another idea could be to use a Data Manager, able to cache data in 
> memory. This would allow a decoupling of data and processing with some 
> maintenance and scalability advantages. The downside is that, unless 
> Node Manager and Processing Server operate in a shared memory 
> environment, the time needed for one component to access data stored 
> in the other, could be prohibitive.
>
> Depending on how we develop these components, it could be fairly easy 
> to adopt a sort of plug in (or loadable modules) architecture. This 
> would allow us to add functionality and/or patch existing routines 
> quite easily and with, possibly, minimal impact on operations. Example 
> of these solutions include the implementation of the VOPS routines in 
> IRAF.
>
> Other things to think about: do we want/need to distribute the various 
> Processing Servers (1 per CCD, maybe) on different machines? Does the 
> quick look need to do processing on all the 4 CCD at the same time 
> (e.g. to derive a global WCS solution)? If so, would it be better to 
> have a 4 CPU machine with 4 fast disks (one per CCD) and 1 or 2 Gb of 
> RAM per CPU that 2 or 4 machines?
>
> I would imagine that the main issue, here, are the speed constraints 
> on the quick look software. Those will probably drive some of the 
> architecture design.
>
>
> Cheers
> Francesco
>
> Notes:
> (1) Just to put some numbers down, a single 1024 x 1024 pixel image, 
> in 64 bit/pixel is 8 Mb. A FAST SCSI Ultra 160 10,000 RPM disk with 8 
> Mb cache (!) would read a single CCD image in less than 0.3 seconds 
> (nominal speed). This means that we would be able to read 9 single CCD 
> images in, approximately, 3 seconds. Of course, these figures assume 
> that the disk operates at either nominal speed of pretty close to 
> that, which is not very realistic. Anyway, assuming a cadence of 30 
> seconds per exposure, and the need to median 9 images to get the 
> background subtraction right, we would spend 10% of the acquisition 
> time, per CCD, just reading the images over and over again. The idea 
> of storing them in RAM appears quite attractive (since 8 images out of 
> 9 would be the same as the previous run).
>
> (2) Assuming we cache (9 images + 9 masks) * 5 filters = 720 Mb per 
> CCD. If the Processing Server could know about the observing strategy, 
> those numbers could be reduced dramatically. This would be the case, 
> for instance, if the Processing Server knew that it does not need to 
> cache data for more than one filter. Another interesting strategy 
> would be to let the Processing Server know that, in some time, the 
> observer will switch filter. In that case, the Processing Server could 
> start to slowly build up a cache of library calibrations for the next 
> filter. Moreover, masks can be compressed quite well.
>
>
>
> On Mar 2, 2004, at 5:16 PM, Frank Valdes wrote:
>
>> The recent IR and NEWFIRM training tutorial put me to thinking at home
>> about the key elements of a NEWFIRM pipeline and data handling system.
>> I needed to write down my ideas while they were in my mind.  The
>> concept that I was thinking about was an optimized program to handle
>> the basic instrumental steps including the difficult problem of
>> tracking the background with a moving median.  With my experience with
>> CCDPROC, the Mosaic Data Handling System, and the distributed Mosaic
>> Pipeline I think the IRPROC processing server is a powerful and
>> implementable concept that I describe in the whitepaper referenced
>> below; it is really more of a design sketch.  The whitepaper may be
>> reached through a recently added DPP NEWFIRM Project Page:
>>
>> http://www.noao.edu/iraflocal/Projects/Newfirm/
>> (account iraf, password iraftwg)
>>
>> The whitepaper direct link (also requiring the account login) is
>>
>> http://www.noao.edu/iraflocal/Projects/Newfirm/irproc.html
>>
>>
>> I hope no one think me presumptous for thinking and writing down ideas
>> such as this.
>>
>> Frank
>>
> ---
> Francesco Pierfederici         <fpierfed@noao.edu>
> NOAO/AURA Inc.                http://www.noao.edu/staff/fpierfed/
> 950 N. Cherry Ave.              Phone: +1 520 318 8402
> Tucson,  AZ 85719 USA       FAX:    +1 520 318 8360
>
>
---
Francesco Pierfederici         <fpierfed@noao.edu>
NOAO/AURA Inc.                http://www.noao.edu/staff/fpierfed/
950 N. Cherry Ave.              Phone: +1 520 318 8402
Tucson,  AZ 85719 USA       FAX:    +1 520 318 8360


